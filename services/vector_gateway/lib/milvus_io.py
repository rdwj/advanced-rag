"""Milvus I/O operations for vector gateway.

Supports Milvus 2.5+ with BM25 sparse vectors and hybrid search.
"""
from __future__ import annotations

import os
from datetime import datetime
from typing import Any, Dict, List
from uuid import uuid4

from pymilvus import (
    AnnSearchRequest,
    DataType,
    Function,
    FunctionType,
    MilvusClient,
    RRFRanker,
)


def get_client() -> MilvusClient:
    """Return a MilvusClient configured from env."""
    uri = os.environ.get("MILVUS_URI")
    host = os.environ.get("MILVUS_HOST", "127.0.0.1")
    port = os.environ.get("MILVUS_PORT", "19530")
    user = os.environ.get("MILVUS_USER")
    password = os.environ.get("MILVUS_PASSWORD")

    connection_uri = uri if uri else f"http://{host}:{port}"

    if user and password:
        return MilvusClient(uri=connection_uri, user=user, password=password)
    return MilvusClient(uri=connection_uri)


def ensure_collection(name: str, dim: int, sparse_dim: int | None = None) -> Dict[str, Any]:
    """Create collection with BM25 + dense vector support for hybrid search.

    Args:
        name: Collection name
        dim: Dense vector dimension (e.g., 1536 for text-embedding-3-small)
        sparse_dim: Ignored (BM25 auto-generates sparse vectors from text)
    """
    client = get_client()
    skip_drop = os.environ.get("MILVUS_SKIP_DROP", "").lower() in {"1", "true", "yes"}
    if client.has_collection(name) and not skip_drop:
        client.drop_collection(name)

    # If collection exists after skip_drop check, just return handle
    if client.has_collection(name):
        return {"client": client, "collection": name}

    schema = client.create_schema(auto_id=False, enable_dynamic_field=False)
    schema.add_field("chunk_id", DataType.VARCHAR, is_primary=True, max_length=64)
    schema.add_field("file_name", DataType.VARCHAR, max_length=256)
    schema.add_field("file_path", DataType.VARCHAR, max_length=512)
    schema.add_field("page", DataType.INT64)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("mime_type", DataType.VARCHAR, max_length=64)
    schema.add_field("created_at", DataType.INT64)
    schema.add_field("chunk_index", DataType.INT64)
    schema.add_field("text", DataType.VARCHAR, max_length=32768, enable_analyzer=True)
    schema.add_field("vector", DataType.FLOAT_VECTOR, dim=dim)
    schema.add_field("sparse_vector", DataType.SPARSE_FLOAT_VECTOR)

    # BM25 function auto-generates sparse vectors from text field
    bm25_function = Function(
        name="text_bm25_emb",
        input_field_names=["text"],
        output_field_names=["sparse_vector"],
        function_type=FunctionType.BM25,
    )
    schema.add_function(bm25_function)

    index_params = client.prepare_index_params()
    index_params.add_index(field_name="vector", index_type="AUTOINDEX", metric_type="COSINE")
    index_params.add_index(field_name="sparse_vector", index_type="SPARSE_INVERTED_INDEX", metric_type="BM25")

    client.create_collection(
        collection_name=name,
        schema=schema,
        index_params=index_params,
    )
    return {"client": client, "collection": name}


def insert_chunks(
    handle: Dict[str, Any],
    chunks: List[Dict[str, Any]],
    vectors: List[List[float]],
    sparse_vectors: List[Dict[int, float]] | None = None,
) -> None:
    """Insert chunks with dense vectors. Sparse vectors auto-generated by BM25 function."""
    if len(chunks) != len(vectors):
        raise ValueError("chunks and vectors must have same length")
    if not chunks:
        return

    vector_dim = len(vectors[0]) if vectors and vectors[0] is not None else 0
    for vec in vectors:
        if len(vec) != vector_dim:
            raise ValueError("all vectors must have the same dimension")

    def _to_timestamp(value: Any) -> int:
        if isinstance(value, (int, float)):
            return int(value)
        if isinstance(value, str):
            try:
                return int(datetime.fromisoformat(value.replace("Z", "+00:00")).timestamp())
            except ValueError:
                return 0
        return 0

    client: MilvusClient = handle["client"]
    collection_name: str = handle["collection"]

    rows = []
    for idx, chunk in enumerate(chunks):
        chunk_index = int(chunk.get("chunk_index", idx))
        base_id = chunk.get("id")
        if not base_id:
            base = chunk.get("file_name", "chunk")[:40]
            base_id = f"{base}-{chunk_index}-{uuid4().hex[:8]}"
        if len(base_id) > 64:
            base_id = f"{base_id[:30]}-{uuid4().hex[:8]}"

        row: Dict[str, Any] = {
            "chunk_id": base_id,
            "file_name": chunk.get("file_name", ""),
            "file_path": chunk.get("file_path", ""),
            "page": chunk.get("page", -1) if chunk.get("page") is not None else -1,
            "section": chunk.get("section", ""),
            "mime_type": chunk.get("mime_type", ""),
            "created_at": _to_timestamp(chunk.get("created_at_ts", chunk.get("created_at"))),
            "chunk_index": chunk_index,
            "text": chunk.get("text", ""),
            "vector": vectors[idx],
        }
        # Note: sparse_vector is auto-generated by BM25 function from text field
        rows.append(row)

    client.insert(collection_name=collection_name, data=rows)
    # Flush to ensure data is immediately visible in stats and queries
    client.flush(collection_name=collection_name)


def hybrid_search(
    collection: str,
    query_vector: List[float],
    query_text: str,
    top_k: int = 5,
    overfetch: int | None = None,
    rrf_k: int = 60,
    nprobe: int = 10,
) -> List[Any]:
    """Execute Milvus hybrid search (dense + BM25) with RRF fusion.

    Args:
        collection: Collection name to search
        query_vector: Dense embedding vector for semantic search
        query_text: Raw query text for BM25 keyword search
        top_k: Number of results to return
        overfetch: Number of candidates to fetch before fusion (default: max(top_k*4, 20))
        rrf_k: RRF parameter (higher = more weight to lower ranks)
        nprobe: Number of clusters to search for dense vectors

    Returns:
        List of search results with entity data and distances
    """
    client = get_client()
    if not client.has_collection(collection):
        raise RuntimeError(f"Milvus collection {collection} not found for hybrid search")

    client.load_collection(collection)
    pool = overfetch or max(top_k * 4, 20)

    # Dense vector search (semantic similarity)
    dense_req = AnnSearchRequest(
        data=[query_vector],
        anns_field="vector",
        param={"metric_type": "COSINE", "params": {"nprobe": nprobe}},
        limit=pool,
    )

    # Sparse vector search (BM25 keyword matching)
    sparse_req = AnnSearchRequest(
        data=[query_text],
        anns_field="sparse_vector",
        param={"metric_type": "BM25"},
        limit=pool,
    )

    # RRF fusion of dense and sparse results
    ranker = RRFRanker(k=rrf_k)

    return client.hybrid_search(
        collection_name=collection,
        reqs=[dense_req, sparse_req],
        ranker=ranker,
        limit=top_k,
        output_fields=[
            "chunk_id",
            "file_name",
            "file_path",
            "page",
            "section",
            "mime_type",
            "created_at",
            "chunk_index",
            "text",
        ],
    )


def search(
    collection: str,
    query_vector: List[float],
    top_k: int = 5,
    nprobe: int = 10,
) -> List[Any]:
    """Execute dense-only vector search (fallback for when query_text not available)."""
    client = get_client()
    if not client.has_collection(collection):
        raise RuntimeError(f"Milvus collection {collection} not found for search")

    client.load_collection(collection)

    return client.search(
        collection_name=collection,
        data=[query_vector],
        anns_field="vector",
        search_params={"metric_type": "COSINE", "params": {"nprobe": nprobe}},
        limit=top_k,
        output_fields=[
            "chunk_id",
            "file_name",
            "file_path",
            "page",
            "section",
            "mime_type",
            "created_at",
            "chunk_index",
            "text",
        ],
    )


def get_context_chunks(
    collection: str,
    file_name: str,
    chunk_index: int,
    window: int = 2,
) -> List[Dict[str, Any]]:
    """Fetch surrounding chunks from the same document.

    Args:
        collection: Collection name
        file_name: Source file to get chunks from
        chunk_index: Center chunk index
        window: Number of chunks before/after to retrieve

    Returns:
        Ordered list of chunks from chunk_index-window to chunk_index+window
    """
    client = get_client()
    if not client.has_collection(collection):
        raise RuntimeError(f"Milvus collection {collection} not found")

    client.load_collection(collection)

    # Query for surrounding chunks
    filter_expr = (
        f'file_name == "{file_name}" && '
        f'chunk_index >= {chunk_index - window} && '
        f'chunk_index <= {chunk_index + window}'
    )

    results = client.query(
        collection_name=collection,
        filter=filter_expr,
        output_fields=[
            "chunk_id",
            "file_name",
            "file_path",
            "page",
            "section",
            "mime_type",
            "chunk_index",
            "text",
        ],
    )

    # Sort by chunk_index
    return sorted(results, key=lambda x: x.get("chunk_index", 0))


def list_collections() -> List[str]:
    """List all available collections."""
    client = get_client()
    return client.list_collections()


def get_or_create_collection(name: str, dim: int = 1536) -> Dict[str, Any]:
    """Get existing collection or create new one (never drops existing data).

    Args:
        name: Collection name
        dim: Dense vector dimension (default: 1536 for text-embedding-3-small)

    Returns:
        Handle dict with client and collection name
    """
    client = get_client()

    # If collection exists, return handle
    if client.has_collection(name):
        return {"client": client, "collection": name}

    # Create new collection with standard schema
    schema = client.create_schema(auto_id=False, enable_dynamic_field=False)
    schema.add_field("chunk_id", DataType.VARCHAR, is_primary=True, max_length=64)
    schema.add_field("file_name", DataType.VARCHAR, max_length=256)
    schema.add_field("file_path", DataType.VARCHAR, max_length=512)
    schema.add_field("page", DataType.INT64)
    schema.add_field("section", DataType.VARCHAR, max_length=256)
    schema.add_field("mime_type", DataType.VARCHAR, max_length=64)
    schema.add_field("created_at", DataType.INT64)
    schema.add_field("chunk_index", DataType.INT64)
    schema.add_field("text", DataType.VARCHAR, max_length=32768, enable_analyzer=True)
    schema.add_field("vector", DataType.FLOAT_VECTOR, dim=dim)
    schema.add_field("sparse_vector", DataType.SPARSE_FLOAT_VECTOR)

    # BM25 function auto-generates sparse vectors from text field
    bm25_function = Function(
        name="text_bm25_emb",
        input_field_names=["text"],
        output_field_names=["sparse_vector"],
        function_type=FunctionType.BM25,
    )
    schema.add_function(bm25_function)

    index_params = client.prepare_index_params()
    index_params.add_index(field_name="vector", index_type="AUTOINDEX", metric_type="COSINE")
    index_params.add_index(field_name="sparse_vector", index_type="SPARSE_INVERTED_INDEX", metric_type="BM25")

    client.create_collection(
        collection_name=name,
        schema=schema,
        index_params=index_params,
    )
    return {"client": client, "collection": name}


def get_collection_stats(collection: str) -> Dict[str, Any]:
    """Get statistics for a collection."""
    client = get_client()
    if not client.has_collection(collection):
        return {"error": f"Collection {collection} not found"}

    # Get entity count
    client.load_collection(collection)
    stats = client.get_collection_stats(collection)

    # Get sample file names
    sample_results = client.query(
        collection_name=collection,
        filter="",
        output_fields=["file_name", "mime_type"],
        limit=100,
    )

    file_names = set()
    mime_types = set()
    for r in sample_results:
        if r.get("file_name"):
            file_names.add(r["file_name"])
        if r.get("mime_type"):
            mime_types.add(r["mime_type"])

    return {
        "name": collection,
        "row_count": stats.get("row_count", 0),
        "file_names": sorted(file_names),
        "mime_types": sorted(mime_types),
    }
